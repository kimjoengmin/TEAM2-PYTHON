## ğŸ”¥ TEAM 2 PYTHON

### ğŸš€ Cell 2 : ì»¬ëŸ¼ëª… ë° ë¬¸ìì—´ ì •ê·œí™”
```python
def standardize_columns(df: pd.DataFrame)
```
- DataFrameì˜ ì»¬ëŸ¼ëª…ê³¼ ë¬¸ìì—´ ê°’ì„ ì†Œë¬¸ì+ì–¸ë”ìŠ¤ì½”ì–´ í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”í•˜ëŠ” í•¨ìˆ˜
#### ğŸ§  ê¸°ëŠ¥
- ì»¬ëŸ¼ëª… ì •ê·œí™” (ì»¬ëŸ¼ëª… ì–‘ìª½ ê³µë°± ì œê±°, ì†Œë¬¸ì ë³€í™˜, ê³µë°±ì„ '_' ëŒ€ì²´, íŠ¹ìˆ˜ë¬¸ì ì œê±°)
```python
df.columns = (
        df.columns
          .str.strip()
          .str.lower()
          .str.replace(' ', '_', regex=False)
          .str.replace(r'[^0-9a-zA-Z_]', '', regex=True)
    )
```
- ë¬¸ìì—´ ë˜ëŠ” ì¹´í…Œê³ ë¦¬ íƒ€ì… ì»¬ëŸ¼ ê°’ ì •ê·œí™” (ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜, ê°’ ì–‘ìª½ ê³µë°± ì œê±°, ê³µë°± '_'ìœ¼ë¡œ ëŒ€ì²´)
```python
for col in df.select_dtypes(include=['object', 'category']):
        df[col] = (
            df[col].astype(str)
                   .str.strip()
                   .str.replace(' ', '_', regex=False)
        )
```

---

### ğŸš€ Cell 3 : ID ë° ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ë†’ì€ ì»¬ëŸ¼ ì œê±°
```python
def drop_id_unnamed_and_missing(df: pd.DataFrame, missing_thresh: float = 0.5)
```
- ID/Unnamed ì»¬ëŸ¼ ë° ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ ì§€ì • ê¸°ì¤€ ì´ìƒì¸ ì»¬ëŸ¼ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜
#### ğŸ§  ê¸°ëŠ¥
- 'Unnamed'ë¡œ ì‹œì‘í•˜ê±°ë‚˜ 'id'ë¡œ ëë‚˜ëŠ” ì»¬ëŸ¼ëª…ì„ ì°¾ê³  ì œê±°
```python
to_drop = df.columns[df.columns.str.contains(r'^(?:unnamed)|id$', case=False, regex=True)]
df = df.drop(columns=to_drop)
```
- ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ê³„ì‚° ë° ë¹„ìœ¨ì´ missing_thresh(**float = 0.5**) ì´ˆê³¼ì¸ ì»¬ëŸ¼ ì œê±°
```python
missing = df.isnull().mean()
df = df.drop(columns=missing[missing > missing_thresh].index)
```

---

### ğŸš€ Cell 4 : ê²°ì¸¡ì¹˜ ëŒ€ì¹˜(Imputation)
```python
def impute_missing(df: pd.DataFrame)
```
- ID/Unnamed ì»¬ëŸ¼ ë° ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ ì§€ì • ê¸°ì¤€ ì´ìƒì¸ ì»¬ëŸ¼ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜
#### ğŸ§  ê¸°ëŠ¥
- ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ëª©ë¡ ì¶”ì¶œ ë° ì¤‘ê°„ê°’(median)ìœ¼ë¡œ ëŒ€ì¹˜
```python
num_cols = df.select_dtypes(include=np.number).columns.tolist()
    if num_cols:
        df[num_cols] = pd.DataFrame(
            SimpleImputer(strategy='median').fit_transform(df[num_cols]),
            columns=num_cols
        )
```
- ë²”ì£¼í˜• ì»¬ëŸ¼ ëª©ë¡ ì¶”ì¶œ ë° ìµœë¹ˆê°’(most_frequent)ìœ¼ë¡œ ëŒ€ì¹˜
```python
cat_cols = df.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()
    if cat_cols:
        df[cat_cols] = pd.DataFrame(
            SimpleImputer(strategy='most_frequent').fit_transform(df[cat_cols]),
            columns=cat_cols
        )
```

---

### ğŸš€ Cell 5 : ì´ìƒì¹˜ ì œê±° í•¨ìˆ˜ (IQR ê¸°ì¤€)
```python
def remove_outliers(df: pd.DataFrame, max_removal: float = 0.2)
```
- IQR(Interquartile Range) ê¸°ì¤€ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ ì œê±°í•˜ë˜, ì „ì²´ ë°ì´í„°ì˜ max_removal ë¹„ìœ¨ ì´í•˜ë§Œ ì œê±°í•˜ëŠ” í•¨ìˆ˜
#### ğŸ§  ê¸°ëŠ¥
- IRQ ê³„ì‚°
```python
for col in df.select_dtypes(include=np.number):
  Q1, Q3 = df[col].quantile([0.25, 0.75])
  IQR = Q3 - Q1
```
- IQR ë²”ìœ„ ë‚´ì— ìˆëŠ” ë°ì´í„°ë§Œ Trueì¸ mask ìƒì„±
```python
mask = df[col].between(Q1 - 1.5 * IQR, Q3 + 1.5 * IQR)
```
- ë‚¨ê²Œ ë  ë°ì´í„°ê°€ ì „ì²´ì˜ (1 - max_removal) ì´ìƒì¸ ê²½ìš°ì—ë§Œ ì´ìƒì¹˜ ì œê±° ìˆ˜í–‰
```python
if mask.sum() > (1 - max_removal) * total:
  df = df[mask]
```

---

### ğŸš€ Cell 6 : ë†’ì€ ìƒê´€ê´€ê³„ ì»¬ëŸ¼ ì œê±°
```python
def drop_highly_correlated(df: pd.DataFrame, threshold: float = 0.95)
```
- ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì¤‘ ìƒê´€ê³„ìˆ˜(ì ˆëŒ“ê°’)ê°€ threshold ì´ìƒì¸ ì»¬ëŸ¼ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜
#### ğŸ§  ê¸°ëŠ¥
- êµ¬ì¹˜í˜• ì»¬ëŸ¼ë¼ë¦¬ì˜ ìƒê´€ê³„ìˆ˜ í–‰ë ¬ ê³„ì‚° (ì ˆëŒ“ê°’ ê¸°ì¤€)
```python
corr = df.select_dtypes(include=np.number).corr().abs()
```
- ìƒì‚¼ê°í–‰ë ¬(Upper Triangle)ë§Œ ì„ íƒ (ì¤‘ë³µ ë¹„êµ ë°©ì§€)
```python
upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))
```
- thresholdë¥¼ ì´ˆê³¼í•˜ëŠ” ìƒê´€ê´€ê³„ê°€ ìˆëŠ” ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
```python
to_drop = [c for c in upper.columns if any(upper[c] > threshold)]
```
- í•´ë‹¹ ì»¬ëŸ¼ë“¤ ì œê±° í›„ ë°˜í™˜
```python
return df.drop(columns=to_drop)
```

---

### ğŸš€ Cell 7 : ë²”ì£¼í˜• ì¸ì½”ë”© ë° ìˆ˜ì¹˜í˜• ìŠ¤ì¼€ì¼ë§
```python
def encode_and_normalize(X: pd.DataFrame, max_onehot: int = 10)
```
- ë²”ì£¼í˜• ë³€ìˆ˜ëŠ” Label/OneHot ì¸ì½”ë”©, ìˆ˜ì¹˜í˜• ë³€ìˆ˜ëŠ” StandardScalerë¡œ ì •ê·œí™”í•˜ëŠ” í•¨ìˆ˜
#### ğŸ§  ê¸°ëŠ¥
- ë²”ì£¼í˜•(object, category, bool) ì»¬ëŸ¼ ì¸ì½”ë”©
```python
for col in X.select_dtypes(include=['object', 'category', 'bool']):
    nun = X[col].nunique()
```
- ê³ ìœ ê°’ì´ 2ê°œ ì´í•˜ â†’ Label Encoding
```python
if nun <= 2:
    X[col] = LabelEncoder().fit_transform(X[col])
```
- ê³ ìœ ê°’ì´ max_onehot ì´í•˜ â†’ One-Hot Encoding
```python
elif nun <= max_onehot:
    ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')
    arr = ohe.fit_transform(X[[col]])
    cols = [f"{col}_{cat}" for cat in ohe.categories_[0]]
    X[cols] = arr
    X = X.drop(columns=[col])
```
- ê³ ìœ ê°’ì´ ë§ìœ¼ë©´ í•´ë‹¹ ì»¬ëŸ¼ ì‚­ì œ
```python
X = X.drop(columns=[col])
```
- ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì •ê·œí™” (StandardScaler ì ìš©)
```python
num_cols = X.select_dtypes(include=np.number).columns
X[num_cols] = StandardScaler().fit_transform(X[num_cols])
```

---

### ğŸš€ Cell 8 : ë°ì´í„° ì „ì²˜ë¦¬ ì „ì²´ í”„ë¡œì„¸ìŠ¤
```python
def preprocess(input_file: str, target_col: str,
               missing_thresh: float = 0.5,
               max_removal: float = 0.2,
               corr_thresh: float = 0.95,
               max_onehot: int = 10) -> str:
    # 1. CSV íŒŒì¼ì„ ì½ì–´ì™€ DataFrameìœ¼ë¡œ ì €ì¥
    df = pd.read_csv(input_file)

    # 2. íƒ€ê²Ÿ ì»¬ëŸ¼ì— ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ì„ ì œê±°í•˜ê³  ì¸ë±ìŠ¤ë¥¼ ì¬ì„¤ì •
    df = df.dropna(subset=[target_col]).reset_index(drop=True)

    # 3. íƒ€ê²Ÿ ë³€ìˆ˜(y)ì™€ í”¼ì²˜(X) ë¶„ë¦¬
    y = df[target_col]
    X = df.drop(columns=[target_col])

    # 4. ì»¬ëŸ¼ëª… ë° ë¬¸ìì—´ ê°’ì„ ì†Œë¬¸ì ë° ì–¸ë”ìŠ¤ì½”ì–´ í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”
    X = standardize_columns(X)

    # 5. 'Unnamed' ë˜ëŠ” 'id' ì»¬ëŸ¼ ë° ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ ë†’ì€ ì»¬ëŸ¼ ì œê±°
    X = drop_id_unnamed_and_missing(X, missing_thresh)

    # 6. ê²°ì¸¡ì¹˜ ëŒ€ì¹˜ (ìˆ˜ì¹˜í˜•: ì¤‘ê°„ê°’, ë²”ì£¼í˜•: ìµœë¹ˆê°’)
    X = impute_missing(X)

    # 7. ì´ìƒì¹˜ ì œê±° (IQR ê¸°ì¤€, ì „ì²´ ë°ì´í„°ì˜ max_removal ë¹„ìœ¨ ì´í•˜ë§Œ ì œê±°)
    X = remove_outliers(X, max_removal)

    # 8. ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì œê±° (ìƒê´€ê³„ìˆ˜ ì ˆëŒ“ê°’ì´ corr_thresh ì´ìƒ)
    X = drop_highly_correlated(X, corr_thresh)

    # 9. ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© ë° ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì •ê·œí™”
    X = encode_and_normalize(X, max_onehot)

    # 10. ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ì œê±°ëœ í–‰ì„ ë°˜ì˜í•˜ì—¬ yì˜ ì¸ë±ìŠ¤ë¥¼ Xì™€ ì¼ì¹˜ì‹œí‚´
    y = y.loc[X.index].reset_index(drop=True)

    # 11. ì›ë³¸ íŒŒì¼ëª…ì—ì„œ í™•ì¥ìë¥¼ ì œê±°í•˜ê³  ìƒˆë¡œìš´ íŒŒì¼ëª… ìƒì„±
    base = os.path.splitext(os.path.basename(input_file))[0]
    out_file = f"processed_{base}.csv"

    # 12. ì „ì²˜ë¦¬ëœ Xì™€ yë¥¼ í•©ì³ ìƒˆë¡œìš´ CSV íŒŒì¼ë¡œ ì €ì¥
    pd.concat([X.reset_index(drop=True), y], axis=1).to_csv(out_file, index=False)

    # 13. ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë¥¼ ì¶œë ¥
    print(f"[INFO] Processed data saved to: {out_file}")

    # 14. ì „ì²˜ë¦¬ëœ íŒŒì¼ ê²½ë¡œ ë°˜í™˜
    return out_file



---

### ğŸš€ Cell 9 : ë©”ì¸ ì‹¤í–‰
```python
input_file = 'íŒŒì¼ ê²½ë¡œ'
processed_path = preprocess(input_file, 'íƒ€ê²Ÿ ì»¬ëŸ¼')
```
